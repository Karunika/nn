{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Model\n",
    "This section for introduction but I really don't know what to write here other than some neural network definition I can pull up from Wikipedia or other blogs, with citation courtousy. So why not you do it yourself? like duh.. ?\n",
    "\n",
    "Well, never mind, I did for ya. Thank me to save you some clicks :)\n",
    "\n",
    "IBM:\n",
    "> Neural networks are a subset of machine learning and are at the heart of deep learning algorithms. Their name and structure are inspired by the human brain, mimicking the way that biological neurons signal to one another.\n",
    "\n",
    "Just in case that doesn't sound super clear\n",
    "\n",
    "## Neuron\n",
    "\n",
    "If we zoom into this extremely complex collosal web of singals transmitting through connections, we see these tiny little units called neurons. During a forward feed these neurons could either be in an active state or an inactive state.\n",
    "\n",
    "Well, actually nah. We can only wish the world was more black and white, 0s and 1s like the bits.\n",
    "\n",
    "Neurons are grey. It could take up a value from anywhere between a lower bound to an upper bound where the higher the value the more active the neuron.\n",
    "\n",
    "And how do we determine these bounds?\n",
    "\n",
    "Well it depends on a lot of factors.\n",
    "\n",
    "## Working in layers\n",
    "The activation of a neuron in a particular layer depends on the weighted sum of the activations of all the neurons in the previous layers.\n",
    "\n",
    "The updated activations $x^{[k]}_i$ of each neuron in a layer $k$ before applying the activation function $\\sigma$ can be given as the weighted sum of the activations of the neurons from the previous i.e., $(k-1)^{th}$ layer plus a bias.\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "w^{[k]}_{11} & w^{[k]}_{12} & w^{[k]}_{13} & \\cdots & w^{[k]}_{1n_{k-1}} \\\\\n",
    "w^{[k]}_{21} & w^{[k]}_{22} & w^{[k]}_{23} & \\cdots & w^{[k]}_{2n_{k-1}} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w^{[k]}_{n_k1} & w^{[k]}_{n_k2} & w^{[k]}_{n_k3} & \\cdots & w^{[k]}_{n_kn_{k-1}} \\\\\n",
    "\\end{bmatrix}\n",
    "\\times\n",
    "\\begin{bmatrix}\n",
    "a^{[k-1]}_1 \\\\\n",
    "a^{[k-1]}_2 \\\\\n",
    "\\vdots \\\\\n",
    "a^{[k-1]}_{n_{k-1}}\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "b^{[k]}_1 \\\\\n",
    "b^{[k]}_2 \\\\\n",
    "\\vdots \\\\\n",
    "b^{[k]}_{n_k}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "z^{[k]}_1 \\\\\n",
    "z^{[k]}_2 \\\\\n",
    "\\vdots \\\\\n",
    "z^{[k]}_{n_k}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Here, $w^{[k]}_{ij}$ represents the weight of the connection from the $j^{th}$ neuron of the $(k-1)^{th}$ layer to the $i^{th}$ neuron of the $k^{th}$ layer, $b^{[k]}_i$ represents the bias associated with the $i^{th}$ neuron of the $k^{th}$ layer, $n_{k-1}$ and $n_k$ are the number of neurons in the $(k-1)^{th}$ and $k^{th}$ layer respectively, and $a^{[k-1]}_j$ represents the value of activation of the $j^{th}$ neuron in the $(k-1)^{th}$ layer.\n",
    "\n",
    "Working with so many variables could get extremely cumbersome and get out of hand in no time. That's why, we represent the above equation in a more compact form i.e., using matrices.\n",
    "\n",
    "$$\n",
    "\\text{A}^{[k]} = \\sigma(Z^{[k]})\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\text{Z}^{[k]} = \\text{W}^{[k]}\\cdot\\text{A}^{[k-1]}+\\text{B}^{[k]}\n",
    "$$\n",
    "\n",
    "\n",
    "## Forward Feed\n",
    "\n",
    "Forward propogation begins with encoding the input data being fed into the model via the input jeurons, followed by running it through all the layers, processing all those signals and applying all sorts of \n",
    "\n",
    "1. Input data activating the input layer neurons\n",
    "2. the input neurons trransmitting that singals into the connections\n",
    "3. the output\n",
    "\n",
    "\n",
    "## Back Propogation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
